# -*- coding: utf-8 -*-
"""MM_Task01_b_text.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11fhjsCoJRnFBgmd0gsrhAeSFPDNtlRgW
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import os

DATA_DIR = "/content/drive/MyDrive/TASK01/data/"

train_df = pd.read_csv(DATA_DIR + "train_event_gap_reduced.csv")
dev_df   = pd.read_csv(DATA_DIR + "dev_event_gap_reduced.csv")
test_df  = pd.read_csv(DATA_DIR + "test_event_gap_reduced.csv")

print("Train:", len(train_df))
print("Dev:", len(dev_df))
print("Test:", len(test_df))

print(train_df.columns)

"""Encode event_name"""

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

train_df["label_id"] = label_encoder.fit_transform(train_df["event_name"])
dev_df["label_id"]   = label_encoder.transform(dev_df["event_name"])
test_df["label_id"]  = label_encoder.transform(test_df["event_name"])

print("Label mapping:")
for i, cls in enumerate(label_encoder.classes_):
    print(i, "->", cls)

print(train_df["label_id"].value_counts())

"""Preprocessing"""

train_df.head(3)

train_df["event_name"].value_counts()

dev_df["event_name"].value_counts()

test_df["event_name"].value_counts()

!pip install emoji

import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
import emoji
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.utils import to_categorical

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

pip install nltk emoji pyspellchecker contractions

import nltk
nltk.download("stopwords")
nltk.download("punkt")
nltk.download('punkt_tab')

import re
import string
import emoji
import contractions
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from spellchecker import SpellChecker

stop_words = set(stopwords.words("english"))
spell = SpellChecker()

def preprocess_text(text):
    # 1. Convert to string and lowercase
    text = str(text).lower()

    # 2. Remove URLs
    text = re.sub(r"http\S+|www\S+", "", text)

    # 3. Remove hashtag symbol (keep the word)
    text = re.sub(r"#", "", text)

    # 4. Convert emojis/emoticons to text
    text = emoji.demojize(text)

    # 5. Expand contractions (abbreviations)
    # e.g. can't → cannot, isn't → is not
    text = contractions.fix(text)

    # 6. Remove non-ASCII characters
    text = re.sub(r"[^\x00-\x7F]+", "", text)

    # 7. Replace punctuation with whitespace
    text = re.sub(f"[{re.escape(string.punctuation)}]", " ", text)

    # 8. Tokenize
    tokens = word_tokenize(text)

    # 9. Remove stopwords
    tokens = [t for t in tokens if t not in stop_words]

    # 10. Correct misspelled words (light correction)
    #tokens = [spell.correction(t) if spell.correction(t) else t for t in tokens]

    # 11. Rejoin tokens
    text = " ".join(tokens)

    # 12. Normalize whitespace
    text = re.sub(r"\s+", " ", text).strip()

    return text

dev_df["clean_text"]   = dev_df["tweet_text"].apply(preprocess_text)

dev_df.to_csv(DATA_DIR + "dev_4class_with_clean_text.csv", index=False)

test_df["clean_text"]  = test_df["tweet_text"].apply(preprocess_text)

test_df.to_csv(DATA_DIR + "test_4class_with_clean_text.csv", index=False)

train_df["clean_text"] = train_df["tweet_text"].apply(preprocess_text)

train_df.to_csv(DATA_DIR + "train_4class_with_clean_text.csv", index=False)

"""Reload cleaned text files"""

train_df = pd.read_csv(DATA_DIR + "train_4class_with_clean_text.csv")
dev_df   = pd.read_csv(DATA_DIR + "dev_4class_with_clean_text.csv")
test_df  = pd.read_csv(DATA_DIR + "test_4class_with_clean_text.csv")

assert train_df["clean_text"].isnull().sum() == 0
assert set(train_df["label_id"].unique()) == {0,1,2,3}

print("Train:", train_df.shape)
print("Dev:", dev_df.shape)
print("Test:", test_df.shape)

print(train_df.columns)

train_df[["tweet_text", "clean_text"]].sample(5, random_state=42)

"""Tokenization"""

!pip install -q transformers datasets accelerate evaluate

import torch
import numpy as np
from transformers import (
    RobertaTokenizer,
    RobertaForSequenceClassification,
    Trainer,
    TrainingArguments
)

MODEL_NAME = "roberta-large"
MAX_LEN = 128

tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)

class TweetDataset(torch.utils.data.Dataset):
    def __init__(self, texts, labels, tokenizer):
        self.encodings = tokenizer(
            texts,
            truncation=True,
            padding="max_length",
            max_length=MAX_LEN
        )
        self.labels = labels

    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = TweetDataset(
    train_df["clean_text"].tolist(),
    train_df["label_id"].tolist(),
    tokenizer
)

dev_dataset = TweetDataset(
    dev_df["clean_text"].tolist(),
    dev_df["label_id"].tolist(),
    tokenizer
)

test_dataset = TweetDataset(
    test_df["clean_text"].tolist(),
    test_df["label_id"].tolist(),
    tokenizer
)

"""Multiclass RoBERTa model"""

model = RobertaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=4
)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)

    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average="macro"
    )
    acc = accuracy_score(labels, preds)

    return {
        "accuracy": acc,
        "precision_macro": precision,
        "recall_macro": recall,
        "f1_macro": f1
    }

training_args = TrainingArguments(
    output_dir="./task1b_text_roberta_4class",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=1e-6,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=10,
    weight_decay=0.01,
    logging_steps=100,
    load_best_model_at_end=True,
    metric_for_best_model="f1_macro",
    greater_is_better=True,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

"""Test Evaluation"""

test_results = trainer.evaluate(test_dataset)
print(test_results)

"""Testing"""

import matplotlib.pyplot as plt

# Extract losses from trainer logs
train_losses = []
eval_losses = []
steps = []

for log in trainer.state.log_history:
    if "loss" in log and "eval_loss" not in log:
        train_losses.append(log["loss"])
        steps.append(log.get("step", None))
    if "eval_loss" in log:
        eval_losses.append(log["eval_loss"])

# Plot
plt.figure(figsize=(8, 5))
plt.plot(train_losses, label="Training Loss")
plt.plot(eval_losses, label="Validation Loss")
plt.xlabel("Evaluation Step")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Predict on validation (dev) set
dev_preds = trainer.predict(dev_dataset)

y_true = dev_preds.label_ids
y_pred = np.argmax(dev_preds.predictions, axis=1)

# Confusion matrix
cm = confusion_matrix(y_true, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap="Blues", values_format="d")
plt.title("Confusion Matrix (Validation Set)")
plt.show()

from sklearn.metrics import classification_report

print(
    classification_report(
        y_true,
        y_pred,
        digits=4
    )
)

"""Test With custom text"""

id2label = {
    0: "earthquake",
    1: "floods",
    2: "hurricane",
    3: "wildfires"
}

def predict_text(text, tokenizer, model, max_len=128):
    model.eval()

    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding="max_length",
        max_length=max_len
    )

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits
    probs = torch.softmax(logits, dim=1).cpu().numpy()[0]
    pred_id = probs.argmax()

    return pred_id, probs

sample_text = "Heavy flooding reported in Colombo after continuous rain"

pred_id, probs = predict_text(sample_text, tokenizer, trainer.model)

print("Input text:")
print(sample_text)
print("\nPredicted class:", id2label[pred_id])
print("\nClass probabilities:")

for i, p in enumerate(probs):
    print(f"{id2label[i]}: {p:.4f}")

"""Save final model"""

MODEL_DIR = "/content/drive/MyDrive/TASK01/models/final_task1b_text_roberta_4class"

import os
os.makedirs(MODEL_DIR, exist_ok=True)

trainer.save_model(MODEL_DIR)
tokenizer.save_pretrained(MODEL_DIR)

import pickle
with open(os.path.join(MODEL_DIR, "label_encoder.pkl"), "wb") as f:
    pickle.dump(label_encoder, f)

print("Model saved to:", MODEL_DIR)

"""LOAD model"""

from transformers import AutoModelForSequenceClassification, AutoTokenizer
import pickle

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)

# Load model
model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)

# Load label encoder
with open(f"{MODEL_DIR}/label_encoder.pkl", "rb") as f:
    label_encoder = pickle.load(f)

print("Model, tokenizer, and label encoder loaded successfully!")

"""test loaded mdl"""

import torch

text = "Heavy flooding reported in Colombo after continuous rain"

inputs = tokenizer(
    text,
    return_tensors="pt",
    truncation=True,
    padding=True
)

with torch.no_grad():
    outputs = model(**inputs)

pred_id = outputs.logits.argmax(dim=1).item()
pred_label = label_encoder.inverse_transform([pred_id])[0]

print("Prediction:", pred_label)

