# -*- coding: utf-8 -*-
"""location.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ogV_tS6qyQuAocFzduEhOGFyA019NNeo
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

DATA_DIR = "/content/drive/MyDrive/TASK01/data/"

# Load CSV
df = pd.read_csv(DATA_DIR + "location_o_.csv")

# Keep only informative rows
df_informative = df[df["label_text"] == "informative"]

# (Optional) reset index
df_informative = df_informative.reset_index(drop=True)

# (Optional) save to a new CSV
df_informative.to_csv(DATA_DIR + "location_o_informative_only.csv", index=False)

print("Original size:", len(df))
print("After filtering:", len(df_informative))

"""Preprocessing"""

import re

def preprocess_tweet_text(text, sentence_segment=False):
    # 1. Convert input to string
    text = str(text)

    # 2. Remove URLs
    text = re.sub(r"http\S+|www\S+", "", text)

    # 3. Remove user mentions (@username)
    text = re.sub(r"@\w+", "", text)

    # 4. Remove hashtag symbol but keep the word
    text = re.sub(r"#(\w+)", r"\1", text)

    # 5. Remove emojis (unicode emoji ranges)
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # emoticons
        "\U0001F300-\U0001F5FF"  # symbols & pictographs
        "\U0001F680-\U0001F6FF"  # transport & map symbols
        "\U0001F700-\U0001F77F"
        "\U0001F780-\U0001F7FF"
        "\U0001F800-\U0001F8FF"
        "\U0001F900-\U0001F9FF"
        "\U0001FA00-\U0001FAFF"
        "]+",
        flags=re.UNICODE
    )
    text = emoji_pattern.sub("", text)

    # 6. Normalize repeated characters (Soooo â†’ Soo)
    text = re.sub(r"(.)\1{2,}", r"\1\1", text)

    # 7. Remove noisy special characters but keep . and ,
    text = re.sub(r"[^A-Za-z0-9\s\.,]", " ", text)

    # 8. Normalize whitespace
    text = re.sub(r"\s+", " ", text).strip()

    # 9. Optional sentence segmentation
    if sentence_segment:
        import nltk
        nltk.download("punkt", quiet=True)
        from nltk.tokenize import sent_tokenize
        text = sent_tokenize(text)

    return text

df["tweet_text_clean"] = df["tweet_text"].apply(preprocess_tweet_text)

# Check column existence
assert "tweet_text" in df.columns, "tweet_text column not found!"

# Show random samples before & after
sample_df = df[["tweet_text", "tweet_text_clean"]].sample(5, random_state=42)

for i, row in sample_df.iterrows():
    print("-----")
    print("ORIGINAL:")
    print(row["tweet_text"])
    print("\nCLEANED:")
    print(row["tweet_text_clean"])

sample_df[["tweet_text", "tweet_text_clean"]].sample(5, random_state=42)

print("Total rows:", len(df))
print("Empty cleaned texts:", (df["tweet_text_clean"].str.len() == 0).sum())
print("Null cleaned texts:", df["tweet_text_clean"].isna().sum())

OUTPUT_PATH = "/content/drive/MyDrive/TASK01/data/location_o_informative_cleaned.csv"

df.to_csv(OUTPUT_PATH, index=False)

print("Saved cleaned file to:", OUTPUT_PATH)

"""extract location names"""

cleaned_df = pd.read_csv(DATA_DIR + "location_o_informative_cleaned.csv")

!pip install -q spacy
!python -m spacy download en_core_web_sm

import spacy
import pandas as pd

nlp = spacy.load("en_core_web_sm")

def extract_locations(text):
    if not isinstance(text, str) or text.strip() == "":
        return []

    doc = nlp(text)

    locations = [
        ent.text for ent in doc.ents
        if ent.label_ in {"GPE", "LOC", "FAC"}
    ]

    # Remove duplicates while preserving order
    seen = set()
    locations = [x for x in locations if not (x in seen or seen.add(x))]

    return locations

cleaned_df["locations"] = cleaned_df["tweet_text_clean"].apply(extract_locations)

cleaned_df[["tweet_text_clean", "locations"]].sample(5, random_state=42)

for _, row in cleaned_df.sample(5, random_state=42).iterrows():
    print("TEXT:", row["tweet_text_clean"])
    print("LOCATIONS:", row["locations"])
    print("-----")

OUTPUT_PATH = "/content/drive/MyDrive/TASK01/data/location_o_informative_cleaned_with_locations.csv"
cleaned_df.to_csv(OUTPUT_PATH, index=False)

print("Saved file to:", OUTPUT_PATH)

"""GEOcode"""

!pip install -q geopy

from geopy.geocoders import Nominatim
from geopy.extra.rate_limiter import RateLimiter

geolocator = Nominatim(user_agent="disaster-ner-geocoder")

# Respect OpenStreetMap usage policy
geocode = RateLimiter(
    geolocator.geocode,
    min_delay_seconds=1,      # 1 request / second
    swallow_exceptions=True
)

geocode_cache = {}

def geocode_locations(location_list):
    results = []

    if not isinstance(location_list, list):
        return results

    for loc in location_list:
        if loc in geocode_cache:
            geo = geocode_cache[loc]
        else:
            geo = geocode(loc)
            geocode_cache[loc] = geo

        if geo is not None:
            results.append({
                "name": loc,
                "latitude": geo.latitude,
                "longitude": geo.longitude
            })

    return results

cleaned_df["locations_geocoded"] = cleaned_df["locations"].apply(geocode_locations)

for _, row in cleaned_df.sample(5, random_state=42).iterrows():
    print("TEXT:", row["tweet_text_clean"])
    print("LOCATIONS:", row["locations"])
    print("GEOCODED:", row["locations_geocoded"])
    print("-----")

cleaned_df["lat"] = cleaned_df["locations_geocoded"].apply(
    lambda x: x[0]["latitude"] if len(x) > 0 else None
)

cleaned_df["lon"] = cleaned_df["locations_geocoded"].apply(
    lambda x: x[0]["longitude"] if len(x) > 0 else None
)

OUTPUT_PATH = "/content/drive/MyDrive/TASK01/data/location_o_final_geocoded.csv"
cleaned_df.to_csv(OUTPUT_PATH, index=False)

print("Saved geocoded file to:", OUTPUT_PATH)

